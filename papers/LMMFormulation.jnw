% !TEX TS-program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = en_US

\documentclass[article]{jss}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage{minted}
\usepackage{mathspec}

% This isn't the best way to do Unicode in general, but it's the best way to do a small
% set of math symbols in text.

%%%% change unicode to math mode sequences
\usepackage{newunicodechar}
\newunicodechar{ρ}{\ensuremath{\rho}}
\newunicodechar{σ}{\ensuremath{\sigma}}
\newunicodechar{×}{\ensuremath{\times}}
\newunicodechar{θ}{\ensuremath{\theta}}
\newunicodechar{Σ}{\ensuremath{\Sigma}}
\newunicodechar{λ}{\ensuremath{\lambda}}
\newunicodechar{Λ}{\ensuremath{\Lambda}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Phillip M. Alday\\Max Planck Institute\And
				Dave Kleinschmidt\\Rutgers University\And
				Reinhold Kliegl\\University of Potsdam\And
				Douglas Bates\\University of Wisconsin}

\title{Efficient Evaluation of a Profiled Log-Likelihood for Linear Mixed-Effects Models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{ Phillip M. Alday,  Dave Kleinschmidt,  Reinhold Kliegl,  Douglas Bates} %% comma-separated
\Plaintitle{Efficient Evaluation of a Profiled Log-Likelihood for Linear Mixed-Effects Models} %% without formatting
%% an abstract and keywords
\Abstract{
  An earlier paper (Bates, Maechler, Bolker and Walker, 2015) showed that
  the profiled log-likelihood of a linear mixed-effects model can be
  evaluated from the solution to a penalized least square problem and
  described in some detail the implementation of this method in the lme4
  package for R. In this paper we described an enhanced approach to this
  PLS expression and its implementation in the MixedModels package for
  Julia.
}
\Keywords{linear mixed models, penalized least squares, Cholesky decomposition, Julia language}
\Plainkeywords{linear mixed models, penalized least squares, Cholesky decomposition, Julia language} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
    Douglas Bates\\
    Department of Statistics\\
    1300 University Ave.\\
    Madison, WI 53706\\
    U.S.A.  \\\email{douglas.bates@wisc.edu}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\hypertarget{introduction}{\section{Introduction}\label{sec:introduction}}

We derive an enhanced form of the representation of the profiled
log-likelihood for linear mixed-effects models (LMMs) given in
\citet{bates.maechler.etal:2015} and describe the implementation of this
approach in the MixedModels package for Julia. This formulation and
implementation are particularly effective when fitting linear mixed
models to large data sets with complex structure \citep{bates2019n}.

In \citet{bates.maechler.etal:2015} the expression for the profiled
log-likelihood was derived from the solution to a penalized least
squares (PLS) problem. We show that it is not necessary to evaluate the
solution completely if only the objective function value is required.
The information necessary to evaluate the profiled
log-likelihood can be obtained from the Cholesky factor of an
extended system. By itself this is not terribly important as most of the
work for the explicit solution takes place in the update of the Cholesky
factor. However, the optimization of the objective to obtain the maximum
likelihood estimates (MLEs) typically involves tens or hundreds,
sometimes thousands, of evaluations of this objective and, in the
formulation given here, an condensed intermediate representation of the
system to be solved can be re-used at each evaluation.

Within the general framework of the penalized least squares (PLS)
representation of the profiled log-likelihood there can be several
computational approaches; ranging from coarse, general approaches such
as that implemented in the lme4 package for R, to fine-grained
approaches that require considerable flexibility in implementation of
individual pieces. Several aspects of the Julia language, such as
multiple dispatch and pass by reference, allow for highly efficient
implementation of the fine-grained approach.

In section~\ref{sec:profiled-log-likelihood} we extend the derivation of the profiled log-likelihood given in \citet{bates.maechler.etal:2015} to an extended PLS problem.
In section~\ref{sec:sparsity-in-random-effects} we demonstrate some of the implementation of the fine-grained
approach to updating the decomposition.

\hypertarget{profiled-log-likelihood-from-an-extended-factorization}{%
\section{Profiled log-likelihood from an extended factorization}\label{sec:profiled-log-likelihood}}

Because the motivation, notation and derivation of the results given here follow so closely those in \citet{bates.maechler.etal:2015} we will simply state them and refer the reader to that paper for a more extensive discussion.  This paper makes one change in notation, the \textit{template} matrix for the $i$th random-effects term, written $\mathbf{T}_i$ in Table 2 of \citet{bates.maechler.etal:2015} is written $\mathbf{\lambda}_i$ here, because it is the template that generates a block of the relative covariance factor, $\mathbf{\Lambda}$.

The probability model for linear mixed-effects incorporates two vector-valued random
variables: the \(n\)-dimensional response, \(\mathcal{Y}\), and the
\(q\)-dimensional random-effects, \(\mathcal{B}\), both of which have
multivariate Gaussian distributions. The mean of the conditional
distribution of \(\mathcal{Y}\) given \(\mathcal{B}=\mathbf{b}\) is a
\emph{linear predictor} incorporating the fixed and known model matrices
\(\mathbf{X}\) (of size \(n\times p\)) and \(\mathbf{Z}\) (of size
\(n\times q\)) and the \(p\)-dimensional fixed-effects parameter vector,
\(\mathbf{\beta}\), as
\begin{equation}\label{eq:Yconddist}
(\mathcal{Y}|\mathcal{B}=\mathbf{b})\sim\mathcal{N}\left(\mathbf{X\beta}+\mathbf{Zb}, \sigma^2\mathbf{I}_n\right)
\end{equation}

(The model in \citet{bates.maechler.etal:2015} incorporated an optional offset, $\mathbf{o}$, and fixed weights matrix, $\mathbf{W}$ in this distribution but they are rarely used for an LMM and we skip them for clarity.)

The unconditional distribution of \(\mathcal{B}\) is
\begin{equation}
\mathcal{B}\sim\mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}\right)
\end{equation}
where the positive semi-definite symmetric covariance matrix \(\mathbf{\Sigma}\), of size \(q\times q\), is parameterized.

For computational convenience, and without loss of generality, the
covariance matrix \(\mathbf{\Sigma}\) is expressed in terms of the
lower-triangular \emph{relative covariance factor},
\(\mathbf{\Lambda}_{\mathbf{\theta}}\), and the scale parameter,
\(\sigma\), as
\begin{equation}\label{eq:spherical}
\mathbf{\Sigma}=\sigma^2\mathbf{\Lambda}_{\mathbf{\theta}}\mathbf{\Lambda}_{\mathbf{\theta}}^\prime .
\end{equation}
(The scale parameter, \(\sigma\), in eqn.~\ref{eq:spherical} is the same as that in eqn.~\ref{eq:Yconddist}.)

These expressions are essentially repetitions of eqn.(2-4) in \citet{bates.maechler.etal:2015}.

As described section 2 of that paper, in lme4 the model matrix $\mathbf{Z}$ is constructed from one or more random-effects terms which induce a sparsity in $\mathbf{Z}$ and both sparsity and repetition in $\mathbf{\Lambda_\theta}$.
The same structure is used in the MixedModels package with the modification that the vertical blocks of $\mathbf{Z}$ and the diagonal blocks of $\mathbf{\Lambda_\theta}$ are indexed by the \textit{grouping factors} instead of the terms in the model formula.
In practice this means that multiple random-effects terms with the same grouping factor are grouped into a composite term when forming the model structure.
All the results and expressions in Tables 2 and 3 of \citet{bates.maechler.etal:2015} applies to MixedModels representation except for the number of parameters, $m_i$, associated with the $i$th block of random effects.

\hypertarget{sleepstudy-example-random-effects-structure}{%
\subsection{Sleepstudy example: random-effects structure}\label{subsec:sleepstudy-restruct}}

The \texttt{sleepstudy} data described in \citet{bates.maechler.etal:2015} are available as a dataset in the \texttt{MixedModels} package
<<label=sleepstudy;echo=true;results="hidden">>=
using BenchmarkTools, DataFrames, MixedModels
sleepstudy = MixedModels.dataset("sleepstudy");
@
<<label=describesleepstudy;line_width=92>>=
describe(sleepstudy)
@

A model that allows for correlated random effects for the intercept and the slope with respect to \texttt{days} for each level of \texttt{subj}, can be fit as
<<label=m1sleepstudy>>=
m1 = fit(MixedModel, @formula(reaction ~ 1+days+(1+days|subj)), sleepstudy)
@

This model has one random-effects term created from $\ell_1=18$ levels of the grouping factor \texttt{subj}
<<label=m1restruct>>=
typeof(only(m1.reterms))
@
(The Julia \code{only} function, like the \code{first} function, returns the first element of a collection, but also checks that the collection has length 1.)
\code{ReMat} is a templated type with the two template arguments, \code{Float64} and \code{2}, describing the element type and the dimension of the random effects for each level of the grouping factor
<<label=m1levels;line_width=72>>=
show(only(m1.reterms).levels)
@
The raw random-effects model matrix $\mathbf{X}_1$ is
<<label=m1rawre;line_width=86>>=
only(m1.reterms).z   # transpose of the raw random-effects model matrix
@
At the estimated $\mathbf{\theta}$
<<label=m1theta>>=
show(m1.theta)
@
the template matrix $\mathbf{\lambda}_1$ is
<<label=m1lambda>>=
only(m1.lambda)      # lower-triangular template matrix T
@  

Another model explored in \citet{bates.maechler.etal:2015} incorporates two random-effect terms to provide uncorrelated random effects for intercept and slope.
<<label=m2>>=
m2 = fit(MixedModel,@formula(reaction ~ 1+days+(1|subj)+(0+days|subj)), sleepstudy)
@

In the \texttt{MixedModels} formulation this model has a single \code{ReMat} structure
<<label=m2remat>>=
typeof(only(m2.reterms))
@
with the same raw random-effects model matrix $\mathbf{X}_1$ as for \code{m1}
<<label=m1m2rawcheck>>=
only(m1.reterms).z == only(m2.reterms).z
@
but a template matrix $\mathbf{\lambda}_1$ constrained to be diagonal
<<label=m2lambda>>=
only(m2.lambda)      # lower-triangular template matrix
@

The elements of the lower-triangular template matrix that are assigned values from $\mathbf{\theta}$ are determined by the \code{inds} field of the \code{ReMat}.
<<label=m2theta>>=
show(m2.theta)
@
<<label=m2inds>>=
show(only(m2.reterms).inds)
@

At this point the details are not as important as simply recognizing that the random-effects model matrix $\mathbf{Z}$ and the relative covariance factor $\mathbf{\Lambda}_\theta$ are generated from blocks corresponding to grouping factors, not to random-effects terms in the model formula.

\hypertarget{penalized-least-squares-through-extended-factorization}{%
\subsection{Penalized least squares through an extended factorization}\label{subsec:extended-penalized-lsq}}

As described in \citet{bates.maechler.etal:2015} the profiled log-likelihood or profiled REML criterion for an LMM can be evaluated from the minimum of a particular penalized least squares (PLS) problem and determinants of Cholesky factors that can be used to solve that problem.

This is based on viewing the relative covariance factor $\mathbf{\Lambda}_\theta$ as generating the random effects vector, \(\mathcal{B}\), with distribution \(\mathcal{N}(\mathbf{0},\Sigma)\), from a
``spherical'' random effects vector, \(\mathcal{U}\), as
\begin{equation}\label{eq:spherical-def}
\mathcal{B} = \mathbf{\Lambda}_{\mathbf{\theta}} \mathcal{U}\quad\mathrm{where}\quad\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I}_q) .
\end{equation}

The joint density of \(\mathcal{Y}\) and \(\mathcal{U}\), which is the product of the conditional density,
\(f_{\mathcal{Y}|\mathcal{U}=\mathbf{u}}(\mathbf{y}|\mathbf{u})\), and
the unconditional density, \(f_{\mathcal{U}}(\mathbf{u})\), becomes
\begin{equation}\label{eq:joint-density}
f_{\mathcal{Y},\mathcal{U}}(\mathbf{y,u})=\frac{1}{(2\pi\sigma^2)^{(n+q)/2}}\exp\left(\frac{-r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})}{2\sigma^2}\right)
\end{equation}
where the penalized sum of squared residuals, \(r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})\), is
\begin{equation}\label{eq:penalized-rss}
\begin{aligned}
  r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})
  &=
  \|\mathbf{y}-\mathbf{X\beta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2 \\
  &=
  \left\|
    \begin{bmatrix}
      \mathbf{Z\Lambda} & \mathbf{X} & \mathbf{y} \\
     -\mathbf{I}_q & \mathbf{0} & \mathbf{0}
    \end{bmatrix}
    \begin{bmatrix}
     -\mathbf{u} \\
     -\mathbf{\beta} \\
     1
    \end{bmatrix}
  \right\|^2 \\
  &=
    \begin{bmatrix}
     -\mathbf{u^\prime} &
     -\mathbf{\beta^\prime} &
      1
    \end{bmatrix}
    \begin{bmatrix}
      \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{X} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{y} \\
      \mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
      \mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
    \end{bmatrix}
    \begin{bmatrix}
     -\mathbf{u} \\
     -\mathbf{\beta} \\
      1
    \end{bmatrix} \\
     &=
    \begin{bmatrix}
     -\mathbf{u^\prime} &
     -\mathbf{\beta^\prime} &
      1
    \end{bmatrix}
    \begin{bmatrix}
      \mathbf{L_{ZZ}} & \mathbf{0} & \mathbf{0} \\
      \mathbf{L_{XZ}} & \mathbf{L_{XX}} & \mathbf{0} \\
      \mathbf{l_{yZ}} & \mathbf{l_{yX}} & l_\mathbf{yy}
    \end{bmatrix}
    \begin{bmatrix}
      \mathbf{L_{ZZ}^\prime} & \mathbf{L_{XZ}^\prime} & \mathbf{l_{yZ}^\prime} \\
      \mathbf{0} & \mathbf{L_{XX}^\prime} & \mathbf{l_{yX}^\prime} \\
      \mathbf{0} & \mathbf{0} & l_\mathbf{yy}
    \end{bmatrix}
    \begin{bmatrix}
     -\mathbf{u} \\
     -\mathbf{\beta} \\
      1
    \end{bmatrix}\\
  &= \left\|
    \begin{bmatrix}
      \mathbf{L_{ZZ}^\prime} & \mathbf{L_{XZ}^\prime} & \mathbf{l_{yZ}^\prime} \\
      \mathbf{0} & \mathbf{L_{XX}^\prime} & \mathbf{l_{yX}^\prime} \\
      \mathbf{0} & \mathbf{0} & l_\mathbf{yy}
    \end{bmatrix}
    \begin{bmatrix}
     -\mathbf{u} \\
     -\mathbf{\beta} \\
      1
    \end{bmatrix}
    \right\|^2\\
  &= \| \mathbf{l_{yZ}^\prime}-\mathbf{L_{XZ}^\prime\beta}-\mathbf{L_{ZZ}\mathbf{u}} \|^2 +
     \| \mathbf{l_{yX}^\prime}-\mathbf{L_{XX}^\prime\beta}\|^2 + l_\mathbf{yy}^2 .
  \end{aligned}
\end{equation}
using the blocked lower-triangular, sparse Cholesky factor,
\begin{equation}\label{eq:Omega}
  \mathbf{\Omega_\theta}=
  \begin{bmatrix}
    \mathbf{\Lambda_\theta^\prime Z^\prime Z\Lambda_\theta+I} & \mathbf{\Lambda_\theta^\prime Z^\prime X} & \mathbf{\Lambda_\theta^\prime Z^\prime y} \\
    \mathbf{X^\prime Z\Lambda_\theta} & \mathbf{X^\prime X} & \mathbf{X^\prime y} \\
    \mathbf{y^\prime Z\Lambda_\theta} & \mathbf{y^\prime X} & \mathbf{y^\prime y}
  \end{bmatrix}
  \begin{bmatrix}
    \mathbf{L_{ZZ}} & \mathbf{0} & \mathbf{0} \\
    \mathbf{L_{XZ}} & \mathbf{L_{XX}} & \mathbf{0} \\
    \mathbf{l_{yZ}} & \mathbf{l_{yX}} & l_\mathbf{yy}
  \end{bmatrix}
  \begin{bmatrix}
    \mathbf{L_{ZZ}^\prime} & \mathbf{L_{XZ}^\prime} & \mathbf{l_{yZ}^\prime} \\
    \mathbf{0} & \mathbf{L_{XX}^\prime} & \mathbf{l_{yX}^\prime} \\
    \mathbf{0} & \mathbf{0} & l_\mathbf{yy}
  \end{bmatrix} .
\end{equation}

From eqn.~\ref{eq:penalized-rss} it is obvious that, for a fixed value of \(\mathbf{\theta}\), the minimum
\(r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})\) is \(l_\mathbf{yy}^2\) and
the conditional estimate of \(\mathbf{\beta}\) satisfies
\begin{equation}
\mathbf{L_{XX}^\prime}\widehat{\mathbf{\beta}}(\mathbf{\theta})=\mathbf{l_{yX}^\prime} .
\end{equation}
The conditional mode, \(\tilde{\mathbf{u}}\), of
\(\mathcal{U}\) given \(\mathcal{Y}=\mathbf{y}\), is the solution to
\begin{equation}\label{eq:condmode}
\mathbf{L_{ZZ}^\prime}\tilde{\mathbf{u}}=\mathbf{l_{yZ}^\prime}-\mathbf{L_{XZ}^\prime}\widehat{\mathbf{\beta}} .
\end{equation}
(Technically, \(\mathbf{\beta}\) and \(\mathbf{\theta}\)
in eqn.~\ref{eq:condmode} are assumed known because this expression
is a statement about distributions. 
In practice, the estimates, \(\widehat{\mathbf{\theta}}\) and \(\widehat{\beta}\), are plugged in
when evaluating the conditional modes or ``best linear unbiased
predictors (BLUPs)'' as they are sometimes called.)

The determinant, $|\mathbf{L_{ZZ}}|$, is the product of the diagonal elements, which must be positive.
Assuming that the fixed-effects model matrix, $\mathbf{X}$, has full column rank (this is checked and, if necessary, adjusted in a pre-processing step), \(\mathbf{L_{XX}}\) also has positive diagonal elements.

To evaluate the likelihood,
\begin{equation}\label{eq:likelihood-abstract}
L(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y}) = \int_\mathbf{u} f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})\, d\mathbf{u}
\end{equation}
we isolate the part of the joint density that depends on \(\mathbf{u}\) and perform a change of variable
\begin{equation}\label{eq:u-system}
\mathbf{v}=\mathbf{L_{ZZ}^\prime u}+\mathbf{L_{XZ}^\prime\beta}-\mathbf{l_{yZ}^\prime} .
\end{equation}
From the properties of the multivariate Gaussian distribution
\begin{equation}\label{eq:likelihood-integral}
\begin{aligned}
  \int_{\mathbf{u}}\frac{1}{(2\pi\sigma^2)^{q/2}}
    \exp\left(-\frac{\|\mathbf{L_{ZZ}^\prime u}+\mathbf{L_{XZ}^\prime\beta}-\mathbf{l_{yZ}^\prime}\|^2}{2\sigma^2}\right)
    \,d\mathbf{u}
  &= \int_{\mathbf{v}}\frac{1}{(2\pi\sigma^2)^{q/2}}
    \exp\left(-\frac{\|\mathbf{v}\|^2}{2\sigma^2}\right)|\mathbf{L_{ZZ}^\prime}|^{-1}\,d\mathbf{v}\\
  &=|\mathbf{L_{ZZ}}|^{-1}
\end{aligned}
\end{equation}
from which we obtain the likelihood as
\begin{equation}\label{eq:likelihood}
  L(\mathbf{\theta},\mathbf{\beta},\sigma)=
  \frac{|\mathbf{L_{ZZ}}|^{-1}}{(2\pi\sigma^2)^{n/2}}
  \exp\left(-\frac{l_\mathbf{yy}^2 + \|\mathbf{L_{XX}^\prime}(\mathbf{\beta}-\widehat{\mathbf{\beta}})\|^2}{2\sigma^2}\right) .
\end{equation}
Setting \(\mathbf{\beta}=\widehat{\mathbf{\beta}}\)
and taking the logarithm provides the estimate of \(\sigma^2\),
given \(\mathbf{\theta}\), as
\begin{equation}\label{eq:sigma-hat}
\widehat{\sigma^2}=\frac{l_\mathbf{yy}^2}{n}
\end{equation}
which gives the \emph{profiled log-likelihood},
\(\ell(\mathbf{\theta}|\mathbf{y})=\log L(\mathbf{\theta},\widehat{\mathbf{\beta}},\widehat{\sigma})\)
as
\begin{equation}\label{eq:profiled-log-likelihood}
-2\ell(\mathbf{\theta}|\mathbf{y})=2\log(|\mathbf{L_{ZZ}}|) +
    n\left(1+\log\left(\frac{2\pi l_\mathbf{yy}^2(\mathbf{\theta})}{n}\right)\right)
\end{equation}

This may seem complicated but, relative to other formulations of the
model, it is remarkably simple.

One of the interesting aspects of this formulation is that it is not
necessary to solve for the conditional estimate of \(\mathbf{\beta}\) or
the conditional modes of the random effects when evaluating the
log-likelihood. The two values needed for the log-likelihood evaluation,
\(2\log(|\mathbf{L}_{ZZ}|)\) and \(l_\mathbf{yy}^2\), are obtained directly from
the diagonal elements of the Cholesky factor.

Furthermore, $\mathbf{\Omega_\theta}$ and, from that, the Cholesky factor, $\mathbf{L_\theta}$ and the objective to be optimized can be evaluated for a given value of $\mathbf{\theta}$ from
\begin{equation}\label{eqn:A}
\mathbf{A} = \begin{bmatrix}
\mathbf{Z}^\prime\mathbf{Z} & \mathbf{Z}^\prime\mathbf{X} & \mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\end{equation}
and $\mathbf{\Lambda_\theta}$.

In the \code{MixedModels} package the \code{LinearMixedModel} struct contains a symmetric blocked array in the \code{A} field and a similarly structured lower-triangular blocked array in the \code{L} field.
Evaluation of the objective simply involves updating the template matrices, $\lambda_i, i=1,\dots,k$ in the \code{ReMat} structures then updating \code{L} from \code{A} and the $\lambda_i$.

\hypertarget{sleepstudy-example-objective-evaluation}{%
\subsection{Sleepstudy example: objective evaluation}\label{subsec:sleepstudy-objective}}

The sizes and structures of the blocks of \code{A} and \code{L} in a \code{LinearMixedModel} object can be displayed as a \code{BlockDescription} struct.
<<label=sleepblockdesc>>=
BlockDescription(m1)
@
(Because the last row of blocks, consisting of $\mathbf{y}^\prime\mathbf{Z}$, $\mathbf{y}^\prime\mathbf{X}$, and $\mathbf{y}^\prime\mathbf{y}$, always has just one row and is stored densely, it is omitted in this display.)

The $(1,1)$ block of $\mathbf{A}$ is itself block diagonal, consisting of $\ell_1$ blocks each of size $p_1\times p_1$, i.e. $2\times2$.
<<label=sleepAblock11>>=
m1.A[Block(1, 1)]
@
and only the diagonal blocks must be stored, in this case as an array of size $2\times2\times18$
<<label=sleepAblock11data>>=
m1.A[Block(1, 1)].data
@

Because these data are from a balanced experiment (i.e. each subject is observed the same number of times at the same numbers of days of sleep deprivation) and there are no missing data values, the diagonal blocks are all the same.
In an unbalanced experiment or observational study this would not be the case.

When all the random effects are generated from a single grouping factor, as in this example, the block structure of $\mathbf{L}$ is the same as that of $\mathbf{A}$.
Furthermore, the $(1,1)$ block of $\mathbf{L}$ also has the uniform block-diagonal structure.
<<label=sleepLblock11>>=
typeof(m1.L[Block(1,1)])
@

Evaluating the objective function for a new $\mathbf{\theta}$ vector involves
\begin{enumerate}
\item Install the elements of $\mathbf{\theta}$ into the appropriate positions in the lower triangle of the $\mathbf{\lambda}_i,i=1,\dots,k$ matrices (here, $k=1$).
\item Copy the blocks of \code{m1.A} to the corresponding blocks of \code{m1.L}
\item Update, in place, the diagonal sub-blocks of \code{m1.L[Block(1,1)]} by premultiplying by $\mathbf{\lambda}_1^\prime$ and postmultiplying by $\mathbf{\lambda}_1$.
"Inflate" (i.e. add 1 to each element of) the diagonal.
\item Postmultiply, in place, each block of columns of \code{m1.L[Block(2,1)]} and \code{m1.L[Block(3,1)]} by $\mathbf{\lambda}_1$
\item Perform the blocked Cholesky factorization of the lower triangle of \code{m1.L}
\end{enumerate}

Each of these steps requires working with only small sub-matrices or small sub-arrays.
For example, the \code{scaleinflate!} method used to perform step 3 in this case is
<<label=scaleinflate!;results="hidden">>=
function scaleinflate!(Ljj::UniformBlockDiagonal{T}, Λj::ReMat{T,S}) where {T,S}
    λ = Λj.λ
    dind = diagind(S, S)
    Ldat = Ljj.data
    for k in axes(Ldat, 3)
        f = view(Ldat, :, :, k)
        lmul!(λ', rmul!(f, λ))
        for i in dind
            f[i] += one(T)  # inflate diagonal
        end
    end
    Ljj
end
@

It is conventional in Julia to give \textit{mutating} functions, which can change the values of one or more of their arguments, a name that ends in ``\code{!}''.
Because $\mathbf{\lambda}$ is lower triangular, the calls to \code{lmul!} and \code{rmul!}, can operate in place on a \code{view} of the $k$th diagonal block.
Especially in versions 1.5.0 and later of Julia, the use of views into arrays is very fast and often allocation free.

The entire sequence to evaluate the objective, given a value of $\mathbf{\theta}$, say
<<label=theta;results="hidden">>=
θ = m1.θ;
@
is very fast
<<label=objective;cache=true>>=
@btime objective(updateL!(setθ!(m1, θ)))
@

\hypertarget{word-recognition-example-scalar-re}{%
\subsection{Word-recognition example: scalar random effects}\label{subsec:word-scalar}}

% Reinhold: Please fill in description of mrk17 or cite a reference.  Phillip or I can help with LaTeX/BibTeX if needed.

The \code{mrk17_exp1} data set
<<label=mrk17data;results="hidden">>=
mrk17 = MixedModels.dataset("mrk17_exp1");
@
provides response times (ms) by 73 subjects (\code{subj}) exposed to 240 items (\code{item}) under different experimental conditions
<<label=mrk17desc>>=
describe(mrk17)
@
The experimental factors are all two-level factors:
the items are words and their word frequency (\code{F}) is dichotomized as low-frequency (\code{LF}) versus high-frequency (\code{HF}), the priming in the experiment is unrelated (\code{unr}) or related (\code{rel}) to the target, and the image quality is clear (\code{clr}) or degraded (\code{deg}).
The \code{lQ} factor is the lagged image quality (i.e. the image quality of the previous target) and \code{lT} is the lagged target status; word (\code{WD}) or non-word (\code{NW}).

Each of these factors is expressed in a $\pm1$ encoding using the Helmert contrast coding.
<<label=mrkcontrasts;results="hidden">>=
contr = Dict(nm => HelmertCoding() for nm in (:F, :P, :Q, :lQ, :lT));
@

Often the response rate (inverse of the response time) is a more suitable metric for the analysis of such data.
An initial model, incorporating scalar random effects for \code{subj} and for \code{item} and expressing the response as a rate per second, is fit as
<<label=m3form;results="hidden">>=
m3form = @formula 1000/rt ~ 1 + F*P*Q*lQ*lT + (1|subj) + (1|item);
m3 = fit(MixedModel, m3form, mrk17, contrasts=contr);
@
Printing of the fixed-effects coefficient estimates, up to fifth order interactions, is suppressed here.
The variance estimates for the random effects are
<<label=m3varcorr>>=
VarCorr(m3)
@
and the block descriptions are
<<label=m3blockdesc>>=
BlockDescription(m3)
@

Notice that even though the term \code{(1|subj)} occurs before \code{(1|item)} in the formula, the terms have been rearranged in the blocks so that the $(1,1)$ corresponds to \code{item}.
The purpose is to make the $(1,1)$ block as large as possible because that block preserves the sparsity of $\mathbf{A}$ in $\mathbf{L}$.
In contrast, the $(2,2)$ block of $\mathbf{A}$ is diagonal but the $(2,2)$ block of $\mathbf{L}$ is dense.
The process of creating the lower Cholesky factor "fills-in" the $(2,2)$ block unless the $(2,1)$ block has the special structure that each column has only one non-zero entry.

In a model like this with multiple simple, scalar, random-effects terms (i.e. those of the form \code{(1|G)} for some grouping factor \code{G}) the $(2,1)$ block of $\mathbf{A}$ is the cross-tabulation of the levels of the first and second grouping factors.
If the levels of the first grouping factor are \textit{nested} within those of the second grouping factor then the $(2,1)$ block has this structure and the $(2,2)$ block of both $\mathbf{A}$ and $\mathbf{L}$ are both diagonal.
Otherwise, some fill-in occurs.

Evaluating the objective for \code{m3} at a new value of $\mathbf{\theta}$ is very fast because the operations on the $(1,1)$ block, which is the largest block, are scalar operations.
For example, the \code{scaleinflate!} method applied to this block just broadcasts an anonymous scalar function over the diagonal elements with \textit{fused dot-broadcasting}.
<<label=scaleinflatem3;results="hidden";eval=false>>=
function scaleinflate!(Ljj::Diagonal{T}, Λj::ReMat{T,1}) where {T}
    Ljjd = Ljj.diag
    Ljjd .= Ljjd .* abs2(only(Λj.λ)) .+ one(T)
    Ljj
end
@

``Dot broadcasting'' means that operators or function calls can be broadcast over the elements of a vector by adding a dot to the name.
``Fused'' refers to fusing the implicit loops so that only one loop is needed in this case.
The details are not terribly important in this case of applying a scalar function over the diagonal of a matrix but in general the point to notice is that a relatively sophisticated operation can be stated concisely while still attaining near optimal performance.

The objective function evaluation is, again, remarkably fast
<<label=m3theta;results="hidden">>=
θ3 = m3.θ;
@
<<label=m3objective;cache=true>>=
@btime objective(updateL!(setθ!(m3, θ3)))
@

\hypertarget{word-recognition-example-vector-re}{%
\subsection{Word-recognition example: vector-valued random effects}\label{subsec:word-vector}}

Combining vector-valued random effects, as in \S~\ref{subsec:sleepstudy-objective}, with multiple, crossed or partially crossed grouping factors, as in \S~\ref{subsec:word-scalar}, can produce linear mixed models that are very difficult to fit.
Nevertheless, \citet{barrlevyscheepers2013a} suggested that such models should be a starting point in many analysis, resulting in considerable effort being expended in trying to fit such models.

The reason for the difficulty in fitting such models is easy to understand; the dimension of $\mathbf{\theta}$ increases with the square of the dimension of the vector-valued random effects.
Optimizing the objective over a higher dimensional $\mathbf{\theta}$ requires more evaluations of the objective and each objective evaluation is requires more time because the sizes of the blocks of $\mathbf{L}$ are larger.

Furthermore, the optimum in such models frequently is on the boundary of the parameter space, corresponding to a singular covariance matrix for the random effects.

A model with random ``slopes'' for each of the experimental factors by \code{subj} and for each of the experimental factors except word frequency (\code{F}) by \code{item} is fit as
<<label=m4mod;results="hidden">>=
m4form = @formula 1000/rt ~ 1+F*P*Q*lQ*lT+(1+P+Q+lQ+lT|item)+(1+F+P+Q+lQ+lT|subj);
m4 = fit(MixedModel, m4form, mrk17, contrasts=contr);
@
<<label=m4varcorr;line_width=88>>=
VarCorr(m4)
@
This model has, in addition to larger blocks,
<<label=m4blockdesc>>=
BlockDescription(m4)
@
a greater dimension of the covariance parameter, $\mathbf{\theta}$.
<<label=m4thetadim>>=
thetalen = (m3 = length(m3.theta), m4 = length(m4.theta))
@

The time required to fit the model increases because each evaluation of the objective takes longer
<<label=m4objective;cache=true>>=
@btime objective(updateL!(setθ!($m4, $(m4.θ))))
@
and because convergence to the optimum requires more evaluations of the objective.
<<label=m4feval>>=
feval = (m3 = m3.optsum.feval, m4 = m4.optsum.feval)
@

Even though this is not a ``maximal'' model, it does converge to a parameter value on the boundary.
<<label=m4boundary>>=
show(m4.theta[iszero.(m4.lowerbd)])
@

The singularity of the relative covariance factors can also be determined from their condition numbers
<<label=m4cond>>=
cond(m4)
@
or via a principal component analysis of the correlation matrices derived from the estimated random-effects covariance matrices.
<<label=m4PCA>>=
m4.PCA
@

%\bibliographystyle{jss}
\bibliography{ZiFPsychLing}


\end{document}
