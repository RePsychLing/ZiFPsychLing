\documentclass[article]{jss}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage{minted}
\usepackage{mathspec}
%\setmathsfont(Digits,Greek)[Uppercase=Plain,Lowercase=Regular,Scale=MatchLowercase]{GFS Porson}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Phillip M. Alday\\Max Planck Institute\And
				Dave Kleinschmidt\\Rutgers University\And
				Reinhold Kliegl\\University of Potsdam\And
				Douglas Bates\\University of Wisconsin}

\title{Efficient Evaluation of a Profiled Log-Likelihood for Linear Mixed-Effects Models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{ Phillip M. Alday,  Dave Kleinschmidt,  Reinhold Kliegl,  Douglas Bates} %% comma-separated
\Plaintitle{Efficient Evaluation of a Profiled Log-Likelihood for Linear Mixed-Effects Models} %% without formatting
%% an abstract and keywords
\Abstract{
  An earlier paper (Bates, Maechler, Bolker and Walker, 2015) showed that
  the profiled log-likelihood of a linear mixed-effects model can be
  evaluated from the solution to a penalized least square problem and
  described in some detail the implementation of this method in the lme4
  package for R. In this paper we described an enhanced approach to this
  PLS expression and its implementation in the MixedModels package for
  Julia.
}
\Keywords{linear mixed models, penalized least squares, Cholesky decomposition, Julia language}
\Plainkeywords{linear mixed models, penalized least squares, Cholesky decomposition, Julia language} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
    Douglas Bates\\
    Department of Statistics\\
    1300 University Ave.\\
    Madison, WI 53706\\
    U.S.A.  \\\email{douglas.bates@wisc.edu}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\hypertarget{introduction}{\section{Introduction}\label{sec:introduction}}

We derive an enhanced form of the representation of the profiled
log-likelihood for linear mixed-effects models given in
\citet{bates.maechler.etal:2015} and describe the implementation of this
approach in the MixedModels package for Julia. This formulation and
implementation are particularly effective when fitting linear mixed
models to large data sets with complex structure \citep{bates2019n}.

In \citet{bates.maechler.etal:2015} the expression for the profiled
log-likelihood was derived from the solution to a penalized least
squares (PLS) problem. We show that it is not necessary to evaluate the
solution completely if only the objective function value is required.
The information necessary to evaluate the profiled
log-likelihood can be obtained from the Cholesky factor of an
extended system. By itself this is not terribly important as most of the
work for the explicit solution takes place in the update of the Cholesky
factor. However, the optimization of the objective to obtain the maximum
likelihood estimates (MLEs) typically involves tens or hundreds,
sometimes thousands, of evaluations of this objective and, in the
formulation given here, an condensed intermediate representation of the
system to be solved can be re-used at each evaluation.

Within the general framework of the penalized least squares (PLS)
representation of the profiled log-likelihood there can be several
computational approaches; ranging from coarse, general approaches such
as that implemented in the lme4 package for R, to fine-grained
approaches that require considerable flexibility in implementation of
individual pieces. Several aspects of the Julia language, such as
multiple dispatch and pass by reference, allow for highly efficient
implementation of the fine-grained approach.

In section~\ref{sec:profiled-log-likelihood} we extend the derivation of the profiled log-likelihood given in \citet{bates.maechler.etal:2015} to an extended PLS problem.
In section~\ref{sec:sparsity-in-random-effects} we demonstrate some of the implementation of the fine-grained
approach to updating the decomposition.

\hypertarget{profiled-log-likelihood-from-an-extended-factorization}{%
\section{Profiled log-likelihood from an extended factorization}\label{sec:profiled-log-likelihood}}

As described in \citet{bates.maechler.etal:2015}, the probability model
for linear mixed-effects incorporates two vector-valued random
variables: the \(n\)-dimensional response, \(\mathcal{Y}\), and the
\(q\)-dimensional random-effects, \(\mathcal{B}\), both of which have
multivariate Gaussian distributions. The mean of the conditional
distribution of \(\mathcal{Y}\) given \(\mathcal{B}=\mathbf{b}\) is a
\emph{linear predictor} incorporating the fixed and known model matrices
\(\mathbf{X}\) (of size \(n\times p\)) and \(\mathbf{Z}\) (of size
\(n\times q\)) and the \(p\)-dimensional fixed-effects parameter vector,
\(\mathbf{\beta}\), as
\begin{equation}\label{eq:Yconddist}
(\mathcal{Y}|\mathcal{B}=\mathbf{b})\sim\mathcal{N}\left(\mathbf{X\beta}+\mathbf{Zb}, \sigma^2\mathbf{I}_n\right)
\end{equation}

The unconditional distribution of \(\mathcal{B}\) is
\begin{equation}
\mathcal{B}\sim\mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}\right)
\end{equation}
where the positive semi-definite symmetric covariance matrix \(\mathbf{\Sigma}\), of size \(q\times q\), is parameterized.

For computational convenience, and without loss of generality, the
covariance matrix \(\mathbf{\Sigma}\) is expressed in terms of the
lower-triangular \emph{relative covariance factor},
\(\mathbf{\Lambda}_{\mathbf{\theta}}\), and the scale parameter,
\(\sigma\), as
\begin{equation}\label{eq:spherical}
\mathbf{\Sigma}=\sigma^2\mathbf{\Lambda}_{\mathbf{\theta}}\mathbf{\Lambda}_{\mathbf{\theta}}^\prime .
\end{equation}
(The scale parameter, \(\sigma\), in eqn.~\ref{eq:spherical} is the same as that in eqn.~\ref{eq:Yconddist}.)

Another way of viewing the covariance factor is that the random effects
vector, \(\mathcal{B}\), with distribution \(\mathcal{N}(\mathbf{0},\Sigma)\), can be generated from a
``spherical'' random effects vector, \(\mathcal{U}\), as
\begin{equation}\label{eq:spherical-def}
\mathcal{B} = \mathbf{\Lambda}_{\mathbf{\theta}} \mathcal{U}\quad\mathrm{where}\quad\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I}_q) .
\end{equation}

The joint density of \(\mathcal{Y}\) and \(\mathcal{U}\), which is the product of the conditional density,
\(f_{\mathcal{Y}|\mathcal{U}=\mathbf{u}}(\mathbf{y}|\mathbf{u})\), and
the unconditional density, \(f_{\mathcal{U}}(\mathbf{u})\), becomes
\begin{equation}\label{eq:joint-density}
f_{\mathcal{Y},\mathcal{U}}(\mathbf{y,u})=\frac{1}{(2\pi\sigma^2)^{(n+q)/2}}\exp\left(\frac{-r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})}{2\sigma^2}\right)
\end{equation}
where the penalized sum of squared residuals, \(r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})\), is
\begin{equation}\label{eq:penalized-rss}
r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})=\|\mathbf{y}-\mathbf{X\beta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2 .
\end{equation}

To this point this derivation is exactly that of \citet{bates.maechler.etal:2015}.
Here's where it changes.
The penalized sum of squared residuals can be written
\begin{equation}\label{eq:prss}
\begin{aligned}
r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta}) &=  \|\mathbf{y}-\mathbf{X\beta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2\\
&=\left\|\begin{bmatrix}
\mathbf{Z\Lambda}&\mathbf{X}&\mathbf{y}\\
-\mathbf{I}_q&\mathbf{0}&\mathbf{0}
\end{bmatrix}\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\right\|^2 \\
&= \begin{bmatrix}-\mathbf{u}&-\mathbf{\beta}&1\end{bmatrix}
\begin{bmatrix}
\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{X} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\\
&=
\begin{bmatrix}-\mathbf{u}&-\mathbf{\beta}&1\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ}^\prime & \mathbf{0} & \mathbf{0} \\
\mathbf{R}_{ZX}^\prime & \mathbf{R}_{XX}^\prime & \mathbf{0} \\
\mathbf{r}_{Zy}^\prime & \mathbf{r}_{Xy}^\prime & r_{yy}
\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\\
&= \left\|
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\right\|^2\\
&=\|\mathbf{r}_{Zy}-\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{R}_{ZZ}\mathbf{u}\|^2+ \|\mathbf{r}_{Xy}-\mathbf{R}_{XX}\mathbf{\beta}\|^2 + r_{yy}^2\\
&=r_{yy}^2+\|\mathbf{R}_{XX}\mathbf{\beta}-\mathbf{r}_{Xy}\|^2+\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy}\|^2
\end{aligned}
\end{equation} where \begin{equation}
\mathbf{R}(\mathbf{\theta})=
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\end{equation} is the upper triangular, right Cholesky factor of the
symmetric, positive definite matrix
\begin{equation}\label{eqn:right-Cholesky}
\begin{bmatrix}
\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{X} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\end{equation}
The sub-matrices on the diagonal, \(\mathbf{R}_{ZZ}\) and \(\mathbf{R}_{XX}\), are upper triangular with, by construction,
non-negative diagonal elements.
In fact, the diagonal elements of \(\mathbf{R}_{ZZ}\) are necessarily positive because
\(\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z}\mathbf{\Lambda}+\mathbf{I}\)
is always positive definite, even when \(\mathbf{\Lambda}\) or
\(\mathbf{Z}\) are rank-deficient. Its determinant,
\(|\mathbf{R}_{ZZ}|\), is the product of its diagonal elements and must
be positive.

Furthermore, for a fixed value of \(\mathbf{\theta}\) the minimum
\(r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})\) is \(r_{yy}^2\) and
the conditional estimate of \(\mathbf{\beta}\) satisfies
\begin{equation}
\mathbf{R}_{XX}\widehat{\mathbf{\beta}}(\mathbf{\theta})=\mathbf{r}_{Xy} .
\end{equation}
The conditional mode, \(\tilde{\mathbf{u}}\), of
\(\mathcal{U}\) given \(\mathcal{Y}=\mathbf{y}\), is the solution to
\begin{equation}\label{eq:condmode}
\mathbf{R}_{ZZ}\tilde{\mathbf{u}}=\mathbf{r}_{Zy}-\mathbf{R}_{ZX}\mathbf{\beta} .
\end{equation}
Technically, \(\mathbf{\beta}\) and \(\mathbf{\theta}\)
in eqn.~\ref{eq:condmode} are assumed known because this expression
is a statement about distributions. In practice, the estimates,
\(\widehat{\mathbf{\theta}}\) and \(\widehat{\beta}\), are plugged in
when evaluating the conditional modes or ``best linear unbiased
predictors (BLUPs)'' as they are sometimes called.

A Cholesky decomposition, \(\mathbf{A}=\mathbf{R}^\prime\mathbf{R}\),
can be written in terms of \(\mathbf{R}\), the upper triangular factor
on the right, or in terms of \(\mathbf{L}=\mathbf{R}^\prime\), the lower
triangular factor on the left. In languages that use column-major
ordering of arrays, such as Julia and R, there is a slight technical
advantage in evaluating the lower triangular factor because the
algorithm works column-wise, as opposed to the evaluation of the upper
triangular factor that works row-wise. However, the penalized least squares problem is
easier to express in terms of \(\mathbf{R}\) so we write the expressions
in terms of \(\mathbf{R}\) even though the actual evaluation uses
\(\mathbf{L}\).

To evaluate the likelihood,
\begin{equation}\label{eq:likelihood-abstract}
L(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y}) = \int_\mathbf{u} f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})\, d\mathbf{u}
\end{equation}
we isolate the part of the joint density that depends on \(\mathbf{u}\) and perform a change of variable to
\begin{equation}\label{eq:u-system}
\mathbf{v}=\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy} .
\end{equation}
From the properties of the multivariate Gaussian distribution
\begin{equation}\label{eq:likelihood-integral}
\begin{aligned}
\int_{\mathbf{u}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-
\frac{\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy}\|^2}{2\sigma^2}\right)\,d\mathbf{u}
&=\int_{\mathbf{v}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-\frac{\|\mathbf{v}\|^2}{2\sigma^2}\right)|\mathbf{R}_{ZZ}|^{-1}\,d\mathbf{v}\\
&=|\mathbf{R}_{ZZ}|^{-1}
\end{aligned}
\end{equation}
from which we obtain the likelihood as
\begin{equation}\label{eq:likelihood}
L(\mathbf{\theta},\mathbf{\beta},\sigma)=\frac{|\mathbf{R}_{ZZ}|^{-1}}{(2\pi\sigma^2)^{n/2}}\exp\left(-
\frac{r_{yy}^2 + \|\mathbf{R}_{XX}(\mathbf{\beta}-\widehat{\mathbf{\beta}})\|^2}{2\sigma^2}\right) .
\end{equation}
If we plug in \(\mathbf{\beta}=\widehat{\mathbf{\beta}}\)
and take the logarithm we can solve for the estimate of \(\sigma^2\),
given \(\mathbf{\theta}\)
\begin{equation}\label{eq:sigma-hat}
\widehat{\sigma^2}=\frac{r_{yy}^2}{n}
\end{equation}
which gives the \emph{profiled log-likelihood},
\(\ell(\mathbf{\theta}|\mathbf{y})=\log L(\mathbf{\theta},\widehat{\mathbf{\beta}},\widehat{\sigma})\)
as
\begin{equation}\label{eq:profiled-log-likelihood}
-2\ell(\mathbf{\theta}|\mathbf{y})=2\log(|\mathbf{R}_{ZZ}|) +
    n\left(1+\log\left(\frac{2\pi r_{yy}^2(\mathbf{\theta})}{n}\right)\right)
\end{equation}

This may seem complicated but, relative to other formulations of the
model, it is remarkably simple.

One of the interesting aspects of this formulation is that it is not
necessary to solve for the conditional estimate of \(\mathbf{\beta}\) or
the conditional modes of the random effects when evaluating the
log-likelihood. The two values needed for the log-likelihood evaluation,
\(2\log(|\mathbf{R}_{ZZ}|)\) and \(r_{yy}^2\), are obtained directly from
the diagonal elements of the Cholesky factor, \(\mathbf{R}\) or, in practice, its transpose, \(\mathbf{L}\).

\hypertarget{Sparsity-in-random-effects-model-matrices}{%
\section{Sparsity in random-effects model matrices}\label{sec:sparsity-in-random-effects}}

The optimization to obtain the maximum likelihood estimates of the model parameters is conceptually quite simple.
The model specification and the data are used to evaluate the model matrices, $\mathbf{Z}$ and $\mathbf{X}$, and the response vector, $\mathbf{y}$ from which an intermediate, symmetric, blocked matrix of "cross-products"
\begin{equation}\label{eqn:A}
\mathbf{A} = \begin{bmatrix}
\mathbf{Z}^\prime\mathbf{Z} & \mathbf{Z}^\prime\mathbf{X} & \mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\end{equation}
is constructed and stored.
As this is a symmetric matrix that will be used to evaluate $\mathbf{L}$, only the lower triangle is stored.

A key to creating efficient algorithms for fitting linear mixed-effects models is exploiting sparsity in $\mathbf{Z}$, the random-effects model matrix, and both sparsity and repetition in $\mathbf{\Lambda}$.

Random effects are associated with the levels of a \emph{grouping factor} representing \emph{observational units} or \emph{experimental units} in the study.
Common examples of grouping factors are \texttt{subject} or \texttt{item} or \texttt{batch}.
They are known sources of variability in the response but, in contrast to \emph{experimental factors}, they are not manipulated or controlled -- merely observed.
In the language of statistical experimental design, these are blocking factors.

The adjective ``random'' actually applies to the levels of these grouping factors in the sense that the levels are not assumed to be repeatable across experiments.

As in lme4, in the MixedModels package a linear mixed-effects model in defined by a formula and a data table.
The formula must contain one or more \emph{random-effects terms} that include a grouping factor and a linear model expression.
A \textit{scalar random-effects term}, such as \texttt{(1|G)}, associates a single random effect with each level of the factor, \texttt{G}.
A \textit{vector-valued random-effects term}, such as \texttt{(1+x|G)}, associates a vector of random effects with each level of \texttt{G}.

The model matrix, $\mathbf{Z}$, and the relative covariance factor, $\mathbf{\Lambda}$, are further divided into blocks according to the grouping factors.
Suppose the model has two distinct grouping factors for random-effects terms, \texttt{G} and \texttt{H}.
Then $\mathbf{Z}$ is the horizontal concatenation of two model matrices stored sparsely in \texttt{ReMat} structures.
The relative covariance factor, $\mathbf{\Lambda}$, is block diagonal.
The diagonal blocks are either a multiple of the identity, for scalar random-effects, or block diagonal with the number of blocks being the number of levels of the grouping factor and each block being a copy of a small matrix $\lambda$. 
All the random-effects terms for a given grouping factor are amalgamated into a single \texttt{ReMat} structure which contains a sparse representation of a set of columns from $\mathbf{Z}$.
A complete description of the mixed-effects formula language is given in \citet{kleinschmidt.etal:2020}.
For the purposes of this description we note that the model matrix $\mathbf{Z}$


$\mathbf{\Lambda}$ and the blocks of $\mathbf{A}$ and $\mathbf{L}$ associated with the random effects are further decomposed into blocks according to the grouping factors for the random-effects terms.
If there is more than one random-effects term for a single grouping factor, these terms are 



For this discussion we will rely on several examples to illustrate how random-effects terms generate model matrices and relative covariance factors.


The data sets used for illustration are primarily from the \texttt{lme4} package for \texttt{R} and are made available in the \texttt{MixedModels} package through its \texttt{dataset} function.

<<label=packages;term=true>>=
using DataFrames, LinearAlgebra, MixedModels # load packages
using MixedModels: dataset        # use datasets from the MixedModels package
@

\hypertarget{A-single-simple-scalar-random-effects-term}{%
\subsection{A model with a single, simple, scalar random-effects term}\label{subsec:single-simple-scalar}
}

The \texttt{dyestuff} data from \citet{davies:1949} provide the yield (g.) of dyestuff in 5 samples from each of 6 batches of an intermediate product, here labelled \texttt{A} to \texttt{F}.
<<label=dyestuff;term=true>>=
dyestuff = dataset(:dyestuff);
describe(dyestuff)
@
The formula in the model fit shown below states that the response, \texttt{yield}, is modeled as an overall mean plus a simple scalar random effect for each level of \texttt{batch}.
<<label=fm1;term=true>>=
fm1 = fit(MixedModel, @formula(yield ~ 1 + (1|batch)), dyestuff)
@
The model matrix, \(\mathbf{Z}\), for a simple, scalar random-effects terms is just the indicator matrix for the levels of the grouping factor; \texttt{batch}, in this case.
<<label=fm1Z;term=true>>=
first(fm1.allterms)
@
The model matrix, $\mathbf{X}$, for the fixed-effects parameters is a single column of 1's.
<<label=fm1X;term=true>>=
fm1.X'  # display the transpose for brevity
@
and the response, $\mathbf{y}$, is obtained directly from the data frame.
<<label=fm1y;term=true>>=
fm1.y'
@

When evalating the log-likelihood through the Cholesky factor (eqn.~\ref{eqn:right-Cholesky}), the model matrices $\mathbf{Z}$ and $\mathbf{X}$, and response, $\mathbf{y}$, appear only in products of the form $\mathbf{Z^\prime Z}$.  The products in the lower triangle are evaluated and stored in a blocked array, $\mathbf{A}$.
<<label=fm1A;term=true>>=
Symmetric(fm1.A, :L)
@
The lower triangle consists of six blocks.  \texttt{Block(1,1)} is $\mathbf{Z^\prime Z}$ and, for scalar random-effects terms, diagonal.  The elements on its diagonal are a frequency table of the levels of the grouping factor.
<<label=fm1A11;term=true>>=
fm1.A[Block(1,1)]
@
\texttt{Block(2,1)} is $\mathbf{Z^\prime X}$ and \texttt{Block(2,2)} is $\mathbf{X^\prime X}$.
The blocks in the last row are $\mathbf{Z^\prime y}$, $\mathbf{X^\prime y}$, and $\mathbf{y^\prime y}$.
<<label=fm1A21;term=true>>=
fm1.A[Block(2,1)]
fm1.A[Block(2,2)]
@

The left Cholesky factor is computed and stored in blocks of the same size and structure.
<<label=fm1L;term=true>>=
LowerTriangular(fm1.L)
fm1.L[Block(1,1)]
@

The matrix $\mathbf{\Lambda}$ is a multiple of the identity for a model with a single scalar random-effects term and is never explicitly formed.
<<label=fm1theta;term=true>>=
theta = only(fm1.theta)  # the theta vector has length 1 in this case
@

To confirm that in the Cholesky factor the diagonal of \texttt{Block(1,1)} of $\mathbf{L}$ is the square root of the diagonal of $\mathbf{\Lambda^\prime Z^\prime Z\Lambda+I}$
<<label=checkdiag;term=true>>=
isapprox(abs2(first(fm1.L[Block(1,1)].diag)),
    first(fm1.A[Block(1,1)].diag) * abs2(theta) + 1)
@


A brief summary of the sizes and types of the blocks in $\mathbf{A}$ and $\mathbf{L}$ is obtained as
<<label=fm1blockdescrip;term=true>>=
BlockDescription(fm1)
@
When the block structures are the same in $\mathbf{A}$ and $\mathbf{L}$, as in this case, only one type is shown.


\hypertarget{Multiple-crossed-simple-scalar-random-effects-terms}{%
\subsection{A model with a multiple, crossed, simple, scalar random-effects terms}\label{subsec:multiple-crossed-simple-scalar}
}

In the \texttt{penicillin} data set, also from \citet{davies:1949}, each \texttt{sample} (6 in total) is tested on each \texttt{plate} measuring the diameter of the more-or-less circular region where bacterial growth is inhibited. 
<<label=penicillin;term=true>>=
penicillin = dataset(:penicillin)
describe(penicillin)
@
The two grouping factors, \texttt{plate} and \texttt{sample}, for the random effects are said to be \textit{crossed}.
A model with an overall mean as a fixed effect and with random effects for each of the grouping factors is fit as
<<label=fm2;term=true>>=
fm2 = fit(MixedModel, @formula(diameter ~ 1 + (1|plate) + (1|sample)), penicillin)
@

When there are multiple grouping factors for the random effects in the model formula each has a set of block in $\mathbf{A}$ and $\mathbf{L}$.
<<label=fm2blockdesc;term=true>>=
BlockDescription(fm2)
@

%\bibliographystyle{jss}
\bibliography{ZiFPsychLing}


\end{document}
