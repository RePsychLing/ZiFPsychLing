---
title: "Efficient Evaluation of the Profiled Log-Likelihood in Linear Mixed-Effects Models"
author:
- name: Phillip M. Alday
  affil: Max Planck Institute
- name: Dave Kleinschmidt
  affil: Rutgers University
- name: Reinhold Kliegl
  affil: University of Potsdam
- name: Douglas Bates
  affil: University of Wisconsin
address:
- Douglas Bates
- Department of Statistics
- 1300 University Ave.
- Madison, WI 53706
- U.S.A.
email: douglas.bates@wisc.edu
date: April 2020
bibliography: ZiFPsychLing
lang: en-US
mainfont: STIXGeneral
monofont: FreeMono
template: jss.latex
mathspec: true
abstract: 'An earlier paper (Bates, Maechler, Bolker and Walker, 2015) showed that the profiled log-likelihood of a linear mixed-effects model can be evaluated from the solution to a penalized least square problem and described in some detail the implementation of this method in the lme4 package for R.  In this paper we described an enhanced approach to this PLS expression and its implementation in the MixedModels package for Julia.'
keywords: linear mixed models, penalized least squares, Cholesky decomposition, Julia language
---

# Introduction

We derive an enhanced form of the representation of the profiled log-likelihood for linear mixed-effects models given in [@bates.maechler.etal:2015] and describe the implementation of this approach in the MixedModels package for Julia.  This formulation and implementation is particularly effective when fitting linear mixed models to large data sets with complex structure [@bates2019n].

In [@bates.maechler.etal:2015] the expression for the profiled log-likelihood was derived from the solution to a penalized least squares (PLS) problem.  We show that it is not necessary to evaluate the solution completely. The information necessary for the profiled log-likelihood evaluation can be obtained from the Cholesky factor of an extended system.  By itself this is not terribly important as most of the work for the explicit solution takes place in the update of the Cholesky factor.  However, the optimization of the objective to obtain the maximum likelihood estimates (MLEs) typically involves tens or hundreds, sometimes thousands, of evaluations of this objective and, in the formulation given here, an condensed intermediate representation of the system to be solved can be re-used at each evaluation.

Within the general framework of the penalized least squares (PLS) representation of the profiled log-likelihood there can be several computational approaches; ranging from coarse, general approaches such as that implemented in the lme4 package for R, to fine-grained approaches that require considerable flexibility in implementation of individual pieces. Several aspects of the Julia language, such as multiple dispatch and pass by reference, allow for highly efficient implementation of the fine-grained approach.

In section 2 we extend the derivation of the profiled log-likelihood given in [@bates.maechler.etal:2015] to an extended PLS problem. In section 3 we demonstrate some of the implementation of the fine-grained approach to updating the decomposition.

# Profiled log-likelihood from an extended factorization

As described in [@bates.maechler.etal:2015], the probability model for linear mixed-effects incorporates two vector-valued random variables: the response. $\mathcal{Y}$, and the random-effects, $\mathcal{B}$, both of which have multivariate Gaussian distributions.  The mean of the conditional distribution of $\mathcal{Y}$ given $\mathcal{B}=\mathbf{b}$ is a *linear predictor* incorporating the fixed and known model matrices $\mathbf{X}$ (of size $n\times p$) and $\mathbf{Z}$ (of size $n\times q$) as
\begin{equation}
(\mathcal{Y}|\mathcal{B}=\mathbf{b})\sim\mathcal{N}\left(\mathbf{X\beta}+\mathbf{Zb}, \sigma^2\mathbf{I}_n\right)
\end{equation}

The unconditional distribution of $\mathcal{B}$ is
\begin{equation}
\mathcal{B}\sim\mathcal{N}\left(\mathbf{0},\mathbf{\Sigma}\right)
\end{equation}
where the positive semi-definite symmetric covariance matrix $\mathbf{\Sigma}$, of size $q\times q$, is parameterized.

For computational convenience, and without loss of generality, the covariance matrix $\mathbf{\Sigma}$ is expressed in terms of the lower-triangular *relative covariance factor*, $\mathbf{\Lambda}_{\mathbf{\theta}}$, and the scale parameter, $\sigma$, as
\begin{equation}
\mathbf{\Sigma}=\sigma^2\mathbf{\Lambda}_{\mathbf{\theta}}\mathbf{\Lambda}_{\mathbf{\theta}}^\prime
\end{equation}

Another way of viewing the covariance factor is that
the random effects vector, $\mathcal{B}$, with distribution $\mathcal{N}(\mathbf{0},\Sigma)$, can be generated from a "spherical" random effects vector, $\mathcal{U}$, as
\begin{equation}
\mathcal{B} = \mathbf{\Lambda}_{\mathbf{\theta}} \mathcal{U}\quad\mathrm{where}\quad\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I}_q) .
\end{equation}

The joint density of $\mathcal{Y}$ and $\mathcal{U}$, which is the product of the conditional density, $f_{\mathcal{Y}|\mathcal{U}=\mathbf{u}}(\mathbf{y}|\mathbf{u})$, and the unconditional density, $f_{\mathcal{U}}(\mathbf{u})$, becomes
\begin{equation}
f_{\mathcal{Y},\mathcal{U}}(\mathbf{y,u})=\frac{1}{(2\pi\sigma^2)^{(n+q)/2}}\exp{\frac{-r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})}{2\sigma^2}}
\end{equation}
where the penalized sum of squared residuals, $r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})$, is
\begin{equation}
r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})=\|\mathbf{y}-\mathbf{X\beta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2 .
\end{equation}

To this point the derivation is exactly that of [@bates.maechler.etal:2015].  Here's where it changes.  The penalized sum of squared residuals can be written
\begin{equation}
\begin{aligned}
r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta}) &=  \|\mathbf{y}-\mathbf{X\beta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2\\
&=\left\|\begin{bmatrix}
\mathbf{Z\Lambda}&\mathbf{X}&\mathbf{y}\\
-\mathbf{I}_q&\mathbf{0}&\mathbf{0}
\end{bmatrix}\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\right\|^2 \\
&= \begin{bmatrix}-\mathbf{u}&-\mathbf{\beta}&1\end{bmatrix}
\begin{bmatrix}
\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{X} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\\
&=
\begin{bmatrix}-\mathbf{u}&-\mathbf{\beta}&1\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ}^\prime & \mathbf{0} & \mathbf{0} \\
\mathbf{R}_{ZX}^\prime & \mathbf{R}_{XX}^\prime & \mathbf{0} \\
\mathbf{r}_{Zy}^\prime & \mathbf{r}_{Xy}^\prime & r_{yy}
\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\\
&= \left\|
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\mathbf{\beta} \\ 1\end{bmatrix}\right\|^2\\
&=\|\mathbf{r}_{Zy}-\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{R}_{ZZ}\mathbf{u}\|^2+ \|\mathbf{r}_{Xy}-\mathbf{R}_{XX}\mathbf{\beta}\|^2 + r_{yy}^2\\
&=r_{yy}^2+\|\mathbf{R}_{XX}\mathbf{\beta}-\mathbf{r}_{Xy}\|^2+\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy}\|^2
\end{aligned}
\end{equation}
where
\begin{equation}
\mathbf{R}(\mathbf{\theta})=
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \mathbf{r}_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \mathbf{r}_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\end{equation}
is the upper triangular, right Cholesky factor of the symmetric, positive definite matrix
\begin{equation}
\begin{bmatrix}
\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{X} & \mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\end{equation}
The sub-matrices on the diagonal, $\mathbf{R}_{ZZ}$ and $\mathbf{R}_{XX}$, are upper triangular.  Furthermore $\mathbf{R}_{ZZ}$ is sparse with positive diagonal elements because $\mathbf{\Lambda}^\prime\mathbf{Z}^\prime\mathbf{Z}\mathbf{\Lambda}+\mathbf{I}$ is always positive definite, even when $\mathbf{\Lambda}$ or $\mathbf{Z}$ are rank-deficient. Its determinant, $|\mathbf{R}_{ZZ}|$, is the product of its diagonal elements and must be positive.

Furthermore, for a fixed value of $\mathbf{\theta}$ the minimum $r^2_\mathbf{\theta}(\mathbf{u},\mathbf{\beta})$ is $r_{yy}^2$ and the conditional estimate of $\mathbf{\beta}$ satisfies
\begin{equation}
\mathbf{R}_{XX}\widehat{\mathbf{\beta}}(\mathbf{\theta})=\mathbf{r}_{Xy} .
\end{equation}
The conditional mode, $\tilde{\mathbf{u}}$, of $\mathcal{U}$ given $\mathcal{Y}=\mathbf{y}$ is the solution to
\begin{equation}
\mathbf{R}_{ZZ}\tilde{\mathbf{u}}=\mathbf{r}_{Zy}-\mathbf{R}_{ZX}\mathbf{\beta}
\end{equation}
Technically, $\mathbf{\beta}$ and $\mathbf{\theta}$ are assumed known because this is a statement about distributions.  In practice, the estimates, $\widehat{\mathbf{\theta}}$ and $\widehat{\beta}$, are plugged in.

A Cholesky decomposition can be written in terms of the lower triangular factor on the left, $\mathbf{L}$, or in terms of $\mathbf{R}$ on the right.  In languages that use column-major ordering of arrays, such as Julia and R, there is a slight technical advantage in evaluating the lower triangular factor because the algorithm works column-wise as opposed to the evaluation of the upper triangular factor that works row-wise.  However, the theory is a bit easier to see in terms of $\mathbf{R}$ so we write the expressions in terms of $\mathbf{R}$ even though the actual evaluation uses $\mathbf{L}$.

To evaluate the likelihood,
\begin{equation}
L(\mathbf{\theta},\mathbf{\beta},\sigma|\mathbf{y}) = \int_\mathbf{u} f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})\, d\mathbf{u}
\end{equation}
we isolate the part of the joint density that depends on $\mathbf{u}$ and perform a change of variable to
\begin{equation}
\mathbf{v}=\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy} .
\end{equation}
From the properties of the multivariate Gaussian distribution
\begin{equation}
\begin{aligned}
\int_{\mathbf{u}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-
\frac{\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\mathbf{\beta}-\mathbf{r}_{Zy}\|^2}{2\sigma^2}\right)\,d\mathbf{u}
&=\int_{\mathbf{v}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-\frac{\|\mathbf{v}\|^2}{2\sigma^2}\right)|\mathbf{R}_{ZZ}|^{-1}\,d\mathbf{v}\\
&=|\mathbf{R}_{ZZ}|^{-1}
\end{aligned}
\end{equation}
from which we obtain the likelihood as
\begin{equation}
L(\mathbf{\theta},\mathbf{\beta},\sigma)=\frac{|\mathbf{R}_{ZZ}|^{-1}}{(2\pi\sigma^2)^{n/2}}\exp\left(-
\frac{r_{yy}^2 + \|\mathbf{R}_{XX}(\mathbf{\beta}-\widehat{\mathbf{\beta}})\|^2}{2\sigma^2}\right)
\end{equation}
If we plug in $\mathbf{\beta}=\widehat{\mathbf{\beta}}$ and take the logarithm we can solve for the estimate of $\sigma^2$, given $\mathbf{\theta}$
\begin{equation}
\widehat{\sigma^2}=\frac{r_{yy}^2}{n}
\end{equation}
which gives the _profiled log-likelihood_, $\ell(\mathbf{\theta}|\mathbf{y})=\log L(\mathbf{\theta},\widehat{\mathbf{\beta}},\widehat{\sigma})$ as
\begin{equation}
-2\ell(\mathbf{\theta}|\mathbf{y})=2\log(|\mathbf{R}_{ZZ}|) +
    n\left(1+\log\left(\frac{2\pi r_{yy}^2(\mathbf{\theta})}{n}\right)\right)
\end{equation}

This may seem complicated but, relative to other formulations of the model, it is remarkably simple.

One of the interesting aspects of this formulation is that it is not necessary to solve for the conditional estimate of $\mathbf{\beta}$ or the conditional modes of the random effects when evaluating the log-likelihood.  The two values needed for the log-likelihood, $2\log(|\mathbf{R}_{ZZ}|)$ and $r_{yy}^2$ are obtained directly from the Cholesky factor.
